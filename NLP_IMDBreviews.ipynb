{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6VCuRMwZFMf",
        "outputId": "3e3f39c6-1fac-4b38-9140-85c7300f832e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review: b\"This was an absolutely terrible movie. Don't be lured in by Christopher Walken or Michael Ironside. Both are great actors, but this must simply be their worst role in history. Even their great acting could not redeem this movie's ridiculous storyline. This movie is an early nineties US propaganda piece. The most pathetic scenes were those when the Columbian rebels were making their cases for revolutions. Maria Conchita Alonso appeared phony, and her pseudo-love affair with Walken was nothing but a pathetic emotional plug in a movie that was devoid of any real meaning. I am disappointed that there are movies like this, ruining actor's like Christopher Walken's good name. I could barely sit through it.\"\n",
            "Label: 0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, Embedding, Flatten, Bidirectional, LSTM, GRU, Dropout, GlobalAveragePooling1D, Conv1D\n",
        "import tensorflow_datasets as tfds\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "(ds_train, ds_val), ds_info = tfds.load(\"imdb_reviews\", split=[\"train\", \"test\"], shuffle_files=True, as_supervised=True, with_info=True)\n",
        "sample = next(iter(ds_train))\n",
        "print(\"Review:\", sample[0].numpy())\n",
        "print(\"Label:\", sample[1].numpy())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKbJ7sQ5ZJPQ",
        "outputId": "d8074b80-afd4-454e-a9af-d8a7491019bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training samples: 25000\n",
            "Number of validation samples: 25000\n"
          ]
        }
      ],
      "source": [
        "train_data, train_labels = [], []\n",
        "for review, label in ds_train:\n",
        "  train_data.append(str(review.numpy()))\n",
        "  train_labels.append(label.numpy())\n",
        "\n",
        "val_data, val_labels = [], []\n",
        "for review, label in ds_val:\n",
        "  val_data.append(str(review.numpy()))\n",
        "  val_labels.append(label.numpy())\n",
        "\n",
        "print(\"Number of training samples:\", len(train_data))\n",
        "print(\"Number of validation samples:\", len(val_data))\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "BUFFER_SIZE = 1024\n",
        "VOCAB_SIZE = 1000\n",
        "EMBED_DIM = 64\n",
        "MAX_SEQ_LEN = 256\n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(train_data)\n",
        "\n",
        "def create_tfds(tokenizer, X, y, padding=False):\n",
        "  if padding:\n",
        "    X = pad_sequences(tokenizer.texts_to_sequences(X), maxlen=MAX_SEQ_LEN, padding=\"post\")\n",
        "  return tf.data.Dataset.from_tensor_slices((X, y)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "\n",
        "ds_train = create_tfds(tokenizer, train_data, train_labels, padding=True)\n",
        "ds_val = create_tfds(tokenizer, val_data, val_labels, padding=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GPLNxS3Z_w-",
        "outputId": "c0488d58-ff8c-473a-d54d-532d137a61dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_7 (Embedding)     (None, 256, 64)           64000     \n",
            "                                                                 \n",
            " lstm_18 (LSTM)              (None, 256, 128)          98816     \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 256, 256)          394240    \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 256, 256)          0         \n",
            "                                                                 \n",
            " lstm_20 (LSTM)              (None, 128)               197120    \n",
            "                                                                 \n",
            " dropout_9 (Dropout)         (None, 128)               0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 129       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 754,305\n",
            "Trainable params: 754,305\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "391/391 [==============================] - 98s 236ms/step - loss: 0.6923 - acc: 0.5361 - val_loss: 0.6857 - val_acc: 0.4986\n",
            "Epoch 2/30\n",
            "391/391 [==============================] - 90s 230ms/step - loss: 0.6884 - acc: 0.5136 - val_loss: 0.6933 - val_acc: 0.5000\n",
            "Epoch 3/30\n",
            "391/391 [==============================] - 91s 232ms/step - loss: 0.6887 - acc: 0.5244 - val_loss: 0.6960 - val_acc: 0.5318\n",
            "Epoch 4/30\n",
            "391/391 [==============================] - 91s 233ms/step - loss: 0.6307 - acc: 0.6392 - val_loss: 0.5392 - val_acc: 0.7615\n",
            "Epoch 5/30\n",
            "391/391 [==============================] - 91s 233ms/step - loss: 0.5714 - acc: 0.7154 - val_loss: 0.5255 - val_acc: 0.7532\n",
            "Epoch 6/30\n",
            "391/391 [==============================] - 91s 233ms/step - loss: 0.4878 - acc: 0.7779 - val_loss: 0.4710 - val_acc: 0.7901\n",
            "Epoch 7/30\n",
            "391/391 [==============================] - 90s 231ms/step - loss: 0.4271 - acc: 0.8137 - val_loss: 0.4258 - val_acc: 0.8123\n",
            "Epoch 8/30\n",
            "391/391 [==============================] - 91s 234ms/step - loss: 0.3865 - acc: 0.8330 - val_loss: 0.3794 - val_acc: 0.8069\n",
            "Epoch 9/30\n",
            "391/391 [==============================] - 91s 232ms/step - loss: 0.3590 - acc: 0.8445 - val_loss: 0.3547 - val_acc: 0.8511\n",
            "Epoch 10/30\n",
            "391/391 [==============================] - 91s 233ms/step - loss: 0.3458 - acc: 0.8537 - val_loss: 0.4125 - val_acc: 0.8102\n",
            "Epoch 11/30\n",
            "391/391 [==============================] - 91s 232ms/step - loss: 0.3311 - acc: 0.8593 - val_loss: 0.3182 - val_acc: 0.8616\n",
            "Epoch 12/30\n",
            "391/391 [==============================] - 91s 232ms/step - loss: 0.3186 - acc: 0.8656 - val_loss: 0.3112 - val_acc: 0.8654\n",
            "Epoch 13/30\n",
            "391/391 [==============================] - 90s 231ms/step - loss: 0.3086 - acc: 0.8744 - val_loss: 0.3167 - val_acc: 0.8600\n",
            "Epoch 14/30\n",
            "391/391 [==============================] - 91s 232ms/step - loss: 0.3013 - acc: 0.8761 - val_loss: 0.3324 - val_acc: 0.8594\n",
            "Epoch 15/30\n",
            "391/391 [==============================] - 90s 232ms/step - loss: 0.2920 - acc: 0.8791 - val_loss: 0.3281 - val_acc: 0.8689\n",
            "Epoch 16/30\n",
            "391/391 [==============================] - 91s 233ms/step - loss: 0.2847 - acc: 0.8833 - val_loss: 0.3207 - val_acc: 0.8565\n",
            "Epoch 17/30\n",
            "391/391 [==============================] - 91s 233ms/step - loss: 0.2757 - acc: 0.8875 - val_loss: 0.3743 - val_acc: 0.8438\n",
            "391/391 [==============================] - 25s 63ms/step - loss: 0.3743 - acc: 0.8438\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37432342767715454, 0.8437600135803223]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "model3 = Sequential()\n",
        "model3.add(Embedding(input_dim=VOCAB_SIZE, input_length=MAX_SEQ_LEN, output_dim=EMBED_DIM))\n",
        "model3.add(LSTM(128, activation='tanh',\n",
        "                recurrent_activation='sigmoid',\n",
        "                recurrent_dropout=0,\n",
        "                unroll=False,\n",
        "                use_bias=True, \n",
        "                return_sequences=True))\n",
        "model3.add(LSTM(256, activation='tanh',\n",
        "                recurrent_activation='sigmoid',\n",
        "                recurrent_dropout=0,\n",
        "                unroll=False,\n",
        "                use_bias=True, \n",
        "                return_sequences=True))\n",
        "model3.add(Dropout(0.2))\n",
        "model3.add(LSTM(128, activation='tanh',\n",
        "                recurrent_activation='sigmoid',\n",
        "                recurrent_dropout=0,\n",
        "                unroll=False,\n",
        "                use_bias=True))\n",
        "model3.add(Dropout(0.1))\n",
        "model3.add(Dense(1, activation=\"sigmoid\"))\n",
        "model3.summary()\n",
        "\n",
        "\n",
        "model3.compile(optimizer=\"rmsprop\", loss=\"binary_crossentropy\", metrics=[\"acc\"])\n",
        "\n",
        "callbacks = tf.keras.callbacks.EarlyStopping(monitor='val_loss',patience=5)\n",
        "\n",
        "model3.fit(ds_train, epochs=30, validation_data=ds_val, callbacks=[callbacks])\n",
        "\n",
        "model3.evaluate(ds_val)\n",
        "\n",
        "# VALIDATION LOSS SHOULD BE LESS THAN 0.3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model3.save('IMDBreviews.h5')"
      ],
      "metadata": {
        "id": "z18Xq6mQKXU9"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP_IMDBreviews.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}